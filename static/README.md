# Title / Mid-sem submission

## About / Folder containing relavant script and model used to tag given samples as M/B (Malware/Benign). Auxiliary folder containing full script and other auxiliary information.


## Execution

Sample prediction:

* python static.py -p /home/data/test/static /home/model/static.model

In general it adheres to the following format for prediction on test data:

* python static.py -p <path to directory containing test data> <path to .model file>


## Command Line Arguments

There are other options as well which take the form python static.py [OPTION]... [FILE]...

Options:
	data dump
		-d  <path to directory containing train data>  <path to dump the training data as .csv file>
	validate
		-v  <path to .dat file>
	train
		-t  <path to .dat file>  <path to dump model as .model file
	predict
		-p  <path to directory containing test data>  <path to .model file>

Use only one option at a time.

More details on the same can be accessed by typing the following command:

* python static.py --help


## Code

The script for running only the prediction can be found under the static folder as static.py. The full script is available under the misc folder as static_full.py.

### Content

All of the sub-modules and functions are contained individually in each of the script files (and hence may see some redundancy). Mainly three categories of functions can be found - generic helper functions used to automate mundane tasks, auxiliary functions used to directly assist the main functions which are in turn used to process the input.

The code contains exhaustive documentation of each of the functions and the role they play.

At the outset, the main() function gets called and as per the options passed (described previously) invokes respective functions to get the task done. Specifically for the task of predicting labels, the extract_features() function, which builds a feature vector using API features and Struture features obtained by calling on get_str_features() and get_api_features(), is invoked per file. These in turn call other helper functions. Once the features are obtained, they are written to an output file by name dynamic.csv as per the prescribed format.

### Requirements

The code was written and tested in Python 3.7.4

For running only the prediction part, only the following modules were used:

> enum, os, sys, pickle, time, numpy

All of the following python packages would be required for executing the script with all its functions:

> enum, os, sys, pickle, time, numpy, sklearn, pandas, time, csv, feature_selector

If not found, these can be installed using standard pip install under the following names respectively:

* pip install [enum os-sys pickle-mixin times numpy scikit-learn pandas python-csv feature_selector]


## Appendix

### Resources

The following were the main resourcesused in the entire process of building the classifier model starting from feature extraction, feature selection and finally classification itself.

> https://www.sciencedirect.com/science/article/pii/S1319157817300149
> https://ieeexplore.ieee.org/document/8667136
> https://www.geeksforgeeks.org/tf-idf-for-bigrams-trigrams/
> https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/
> https://pypi.org/project/feature-selector/
> Various other online resources and class material


### Process description

From among the three given files for each sample viz., Opcodes.txt, String.txt and Structure_Info.txt mainly the Structure_Info.txt was found very useful. Since the training data contained both packed as well as non-packed samples, the Opcodes and String information didn't really make much of a sense in this case.
Referring to various online resources and class material, the following categories of features were chosen to be extracted from the training samples:

* API features
* Structural features

TF-IDF was used to reduce API features and a specialised feature selection algorithm to select from all of the remaining features was used (more details in links under Resources section). The feature-selector package, which is a tool for dimensionality reduction for machine learning datasets, was used for further feature selection. It helped in feature selection based on single-valuedness, correlation and relative importance of features.

Initially the total number of features were 12173 in number (majorly APIs which were themselves more than 12100 in number). After TF-IDF it was reduced to 240 in number. After applying another feature selection algorithm the total number of features were finally reduced to 165.

#### API features

Initially a set of unique APIs invoked in all of the samples were enlisted (separately for packed and non-packed samples). This was then processed through TF-IDF numerical statistics and top 100 APIs from each of the category (packed and non-packed) were selected. Finally after applying a feature selection algorithm it was further reduced.

#### Structural features

The initial structural features were chosen as per the papers referenced and various online resources on malware detection. After feature selection the number of structural features were reduced.

#### Model selection

From among the standard machine learning models for binary classification such as tree based approaches, bayesian networks, SVM etc, the tree based classifiers produced best results. They are especially robust to over-fitting (since a large number of trees cancel out the negative effect). Among these RandomForestClassifier from the scikit-learn package was used.

#### Intermediate results

The details of all the features initially chosen and the intermediate features as described above can be found under misc/static_features.txt
