import csv
import enum
import os
import pickle
import time
import sys
import numpy as np
import pandas as pd

from sklearn import metrics
from sklearn.datasets.base import Bunch
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import KFold

from feature_selector import FeatureSelector

'''
helper classes
'''

class timer():

    def start(self):
        self.start_time = time.time()

    def end(self):
        end_time = time.time()
        total_time = self.start_time - end_time
        self.hours = int(total_time / 3600)
        remaining_secs = total_time - self.hours * 3600
        self.minutes = int(remaining_secs / 60)
        self.seconds = remaining_secs - self.minutes * 60

    def show(self):
        print("Total Execution Time\n{}h : {}m : {}s".format(self.hours, self.minutes, int(self.seconds)))

class TYPE(enum.Enum):
    BENIGN = 0
    MALWARE = 1

'''
further reduced features after feature selection - total 165
'''
api_features = ['PtInRect', 'GetFileSize', 'GetCurrentThreadId', 'LoadLibraryW', 'GetPropA', 'TlsSetValue', 'GetVersionExA', 'GetLocalTime', 'GetSystemTime', 'SetUnhandledExceptionFilter', 'GetExitCodeProcess', 'send', 'QueryPerformanceCounter', 'GetVersion', 'GetFileAttributesA', 'GetStdHandle', 'VariantCopy', 'GetDlgItem', 'GetACP', 'SysFreeString', 'GetModuleHandleA', 'LoadIconA', 'CharNextW', 'CoInitialize', 'SHChangeNotify', 'GetCommandLineA', 'ShowWindow', 'SetBkMode', 'FreeSid', 'VarUI4FromStr', 'GetCurrentProcessId', 'GetModuleHandleW', 'DeleteObject', 'LoadLibraryA', 'RegOpenKeyA', 'ExitThread', 'FindClose', 'GetStartupInfoA', 'GetTickCount', 'VirtualFree', 'VariantChangeTypeEx', 'RegCloseKey', 'CoTaskMemFree', 'InitializeCriticalSectionAndSpinCount', 'LocalFree', '__WSAFDIsSet', 'FindFirstFileA', 'GlobalFree', 'ReadFile', 'AreFileApisANSI', 'ExitWindowsEx', 'GetFileTitleA', 'GetModuleFileNameA', 'UnhandledExceptionFilter', 'WaitForSingleObject', 'wsprintfA', 'PostMessageA', 'RegQueryValueExA', 'VirtualAlloc', 'ShellExecuteA', 'InitCommonControls', 'SendMessageA', 'VirtualProtectEx', 'ExitProcess', 'InternetOpenA', 'InitializeCriticalSection', 'GdipFree', 'GetConsoleMode', 'RegOpenKeyExA', 'LoadLibraryExA', 'GetProcAddress', 'HeapFree', 'MessageBoxA', 'WriteFile', 'WideCharToMultiByte', 'RegEnumKeyA', 'SetStdHandle', 'RtlUnwind', 'GetLastError', 'InitCommonControlsEx', 'RaiseException', 'CreateFileW', 'CloseHandle', 'Sleep', 'ChooseColorA', 'CreateFontA', 'IsValidCodePage', 'VirtualQuery', 'LocalAlloc', 'SetWindowTextA', 'RegFlushKey', 'TlsAlloc', 'DeleteFileA', 'GetCommandLineW', 'SetLastError', 'MultiByteToWideChar', 'VariantClear', 'GetSystemMetrics', 'VerQueryValueA', 'GetSystemTimeAsFileTime', 'SetFilePointer', 'VirtualProtect', 'lstrlenA', 'SaveDC', 'ShellExecuteW', 'LeaveCriticalSection', 'TerminateProcess', 'SetEndOfFile', 'TlsFree', 'SHGetMalloc', 'FreeLibrary', 'SetErrorMode', 'GetCurrentProcess', 'DispatchMessageA', 'timeGetTime', 'GlobalAlloc', 'LoadCursorA', 'PostMessageW', 'GetWindowsDirectoryA', 'exit', 'CreateProcessA', 'CreateDirectoryA', 'FlushFileBuffers', 'CreateThread', 'WSAStartup', 'GetModuleFileNameW', 'CreateFileA', '_lopen', 'BitBlt', 'GetProcessHeap', 'DestroyWindow', 'GetErrorInfo', 'SetTimer', 'PropertySheetW', 'GetDC', 'FindWindowA']

raw_features = ['e_lfanew', 'NumberOfSections', 'MajorLinkerVersion', 'MinorLinkerVersion', 'SizeOfCode', 'SizeOfInitializedData', 'SizeOfUninitializedData', 'AddressOfEntryPoint', 'BaseOfCode', 'BaseOfData', 'MajorOperatingSystemVersion', 'MinorOperatingSystemVersion', 'MajorImageVersion', 'MinorImageVersion', 'MajorSubsystemVersion', 'CheckSum', 'Subsystem', 'SizeOfStackReserve', 'SizeOfStackCommit', 'SizeOfHeapReserve', 'ImageBase', 'SectionAlignment', 'FileAlignment', 'SizeOfImage', 'SizeOfHeaders']
derived_features = ['StringFileInfo', 'Characteristics', 'DllCharacteristics']
aux_derived = ['[StringFileInfo]', '[IMAGE_FILE_HEADER]', '[IMAGE_OPTIONAL_HEADER]']
str_features = raw_features + derived_features

feature_names = str_features + api_features + ['IsPacked']

'''
generic helper functions
'''

'''
@input
    data_file_path  - input .csv file to process (with meta_data in first row)
    [saveto_path]   - absolute path to save the modified .csv file
@output
    path to modified .csv file (with meta_data removed)
@function
    removes first line of .csv file (containing meta_data) and dumps modified data to another .csv file
'''
def strip_header(data_file_path, saveto_path="Default"):

    if saveto_path == "Default":
        saveto_path = replace_ext(data_file_path, '_mod.csv')

    with open(data_file_path, 'r') as fh:
        fh.readline()
        contents = fh.read()
    with open(saveto_path, 'w') as fh:
        fh.write(contents)

    return saveto_path


'''
@input
    file_path   - absolute file path
    ext         - extension to replace existing one
@output
    default file_path obtained by replacing default extension of file_path with ext
@function
    helps generate default file_path (which is used to save useful information output by other functions)
'''
def replace_ext(file_path, ext):
    (dir_path, file_name) = os.path.split(file_path)
    return dir_path + os.path.sep + str(file_name.split('.')[0]) + ext


'''
@input
    data_file_path  - absolute path to the .csv file containing original train data (features and labels)
    [saveto_path]   - absolute path to the updated .dat file (defaults to '*.dat' in the current directory, * corresponding to original file name)
@output
    dumps the train data to a .dat file and returns absolute path to it
@function
    extracts all training data (features and labels) and packs it into a dataset of numpy array and other auxiliary information (for conveniently loading later)   
'''
def create_dataset(data_file_path, saveto_path="Default"):
    if saveto_path == "Default":
        saveto_path = replace_ext(data_file_path, '.dat')
    with open(data_file_path) as fh:
        file_data = csv.reader(fh)
        # dimensions
        meta_data = next(file_data)
        feature_names = next(file_data)
        # number of data rows, excluding header
        num_samples = int(meta_data[0])
        # number of columns for features, excluding target column
        num_features = int(meta_data[1])
        target_names = ['Benign', 'Malware']
        data = np.empty((num_samples, num_features))
        target = np.empty((num_samples,), dtype=np.int)
        for ind, sample in enumerate(file_data):
            data[ind] = np.asarray(sample[:-1], dtype=np.float64)
            target[ind] = np.asarray(sample[-1], dtype=np.int)
        dataset = Bunch(data=data, target=target, feature_names=feature_names, target_names=target_names,
                        shape=(num_samples, num_features))
        with open(saveto_path, 'wb') as fh1:
            pickle.dump(dataset, fh1)
        return saveto_path


'''
@input
    dat_file    - absolute path to .dat file containing the training data
@output
    the loaded dataset (containing features, labels and other auxialiary information)
'''
def load_dataset(dat_file_path):
    with open(dat_file_path, 'rb') as fh:
        data = pickle.load(fh)
    return data


'''
auxiliary helper functions
'''

'''
@misc
    Call on folders with samples from non-packed executables only
@input
    dir_path        - absolute path to the parent directory containing Malware/ Benign sample folders
    [saveto_path]   - path to save the created data.csv (defaults to api_1-gram.csv)
@function
    extracts 1024 most important API calls and dumps to a .csv file api features of all samples ranked based on TFIDF
'''
def dump_api_info(dir_path, saveto_path="api_1-gram.csv"):
    # prepare opcode_sequences for all files
    api_seq_list = []
    cnt = 0
    for sup_dir in os.listdir(dir_path):
        # .../Malware or .../Benign
        sup_dir = os.path.join(dir_path, sup_dir)
        for sub_dir in os.listdir(sup_dir):
            # .../Malware/Trojan or .../Benign/Only
            sub_dir = os.path.join(sup_dir, sub_dir)
            # 0aeb1...
            for file_dir in os.listdir(sub_dir):
                cnt += 1
                if cnt % 256 == 0:
                    print("Processed {} files".format(cnt))
                # .../Malware/Trojan/0aeb1...
                child_dir = os.path.join(sub_dir, file_dir)
                api_seq = " ".join(find_apis(child_dir))
                if api_seq:
                    api_seq_list.append(api_seq)
    print("Processing tf_idf...")
    # apply tf_idf
    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), lowercase=False)
    new_term_freq_matrix = tfidf_vectorizer.fit_transform(api_seq_list)
    features = (tfidf_vectorizer.get_feature_names())
    # Getting top ranking features
    sums = new_term_freq_matrix.sum(axis=0)
    tfidf_data = []
    for col, term in enumerate(features):
        tfidf_data.append((term, sums[0, col]))
    ranking = pd.DataFrame(tfidf_data, columns=['term', 'ranks'])
    min = ranking.ranks.min()
    filtered_ranking = ranking[ranking.ranks != min]
    rows = (filtered_ranking.sort_values('ranks', ascending=False))
    rows.head(1024).to_csv(saveto_path)


'''
@input
    dir_path        - absolute path to the parent directory containing Malware/ Benign sample folders
    [saveto_path]   - path to save the created data.csv
@function
    creates a .csv file containing structural feature vectors of all samples
'''
def dump_structure_info(dir_path, saveto_path="structure.csv"):
    # .../Malware
    with open(saveto_path, 'w') as fh:
        csv_writer = csv.DictWriter(fh, fieldnames=str_features)
        csv_writer.writeheader()
        cnt = 0
        for sup_dir in os.listdir(dir_path):
            # .../Malware or .../Benign
            sup_dir = os.path.join(dir_path, sup_dir)
            for sub_dir in os.listdir(sup_dir):
                feature_vecs = []
                # .../Malware/Trojan or .../Benign/Only
                sub_dir = os.path.join(sup_dir, sub_dir)
                # 0aeb1...
                for file_dir in os.listdir(sub_dir):
                    cnt += 1
                    if cnt % 256 == 0:
                        print("Processed {} files".format(cnt))
                    # .../Malware/Trojan/0aeb1...
                    feature_vecs.append(get_str_features(os.path.join(sub_dir, file_dir)))
                csv_writer.writerows(feature_vecs)


'''
@input
    data_file_path  - input .csv file to process
    [saveto_path]   - absolute path to save the modified .csv file
@function   
    reduces the number of features by elimination based on correlation, importance of features and dumps modified data to another .csv file
'''
def select_best_features(data_file_path, saveto_path="Default"):

    mod_data_file_path = strip_header(data_file_path)

    if saveto_path == "Default":
        saveto_path = replace_ext(data_file_path, '_reduced.csv')

    X = pd.read_csv(mod_data_file_path)
    y = X['Label']
    X = X.drop(columns=['Label'])

    feature_selector = FeatureSelector(data=X, labels=y)
    feature_selector.identify_single_unique()
    feature_selector.identify_collinear(correlation_threshold=0.98)
    feature_selector.identify_zero_importance(task = 'classification', eval_metric = 'auc',
                            n_iterations = 10, early_stopping = True)
    features_1hot = feature_selector.one_hot_features
    features_base = feature_selector.base_features
    feature_selector.identify_low_importance(cumulative_importance = 0.99)

    X_dash = feature_selector.remove(methods= ['single_unique', 'collinear', 'zero_importance', 'low_importance'], keep_one_hot=False)
    X_dash['Label'] = y

    X_dash.to_csv(saveto_path, index=False)

    meta_data = [str(X_dash.shape[0]), str(X_dash.shape[1] - 1)]
    with open(saveto_path, 'r') as fh:
        contents = fh.read()
    contents = ','.join(meta_data) + '\n' + contents
    with open(saveto_path, 'w') as fh:
        fh.write(contents)

    os.system("rm -f " + mod_data_file_path)


'''
@input
        X           - train data of features
        y           - train data of labels corresponding to X
        clf         - specified classifier on which to train
    [saveto_path]   - absolute path of file to dump the model to (defaults to 'data.model')
@output
    absolute path to the file containing dump of the model 
@function
    trains clf on given data (X, y) and dumps the model onto a file
'''
def train(X, y, clf, saveto_path="Default"):
    if saveto_path == 'Default':
        saveto_path = os.path.join(os.getcwd(), 'static.model')
    clf.fit(X, y)
    with open(saveto_path, 'wb') as fh:
        pickle.dump(clf, fh)
    return saveto_path


'''
@input
    model_path  - absolute path to the file containing dump of the model
    X_test      - test data features
    y_test      - test data labels
@function
    predicts the labels on given test data features and prints prediction metrics 
'''
def predict_(model_path, X_test, y_test):
    with open(model_path, 'rb') as fh:
        model = pickle.load(fh)
    y_pred = model.predict(X_test)
    print("Accurate to ", metrics.accuracy_score(y_test, y_pred),"\n")


'''
@input
    dat_file   - absolute path to the file containing 
    k          - 'k' in k-cross validation (no of splits)
    clf        - classifier to validate on
@function
    implementation of standard k-cross validation
'''
def k_cross_validate(X, y, clf, k):
    k_fold = KFold(n_splits=k, shuffle=True, random_state=1)
    for train_ind, test_ind in k_fold.split(X):
        print("TRAIN:", train_ind, "TEST:", test_ind)
        X_train, X_test = X[train_ind], X[test_ind]
        y_train, y_test = y[train_ind], y[test_ind]
        model_path = train(X_test, y_test, clf, saveto_path='val.model')
        predict_(model_path, X_test, y_test)
        os.system("rm " + model_path)


'''
@function
    usage helper function
'''
def display_help(category=None):
    string = "Incorrect Usage: "
    if category == None:
        string = "Usage:\tpython static.py [OPTION]... [FILE]...\nTag given data-set with labels M/B (Malware/Benign)\n\n" \
                 "--help  display this help info\n\nOptions:\n\t" \
                 "data dump\n\t\t-d  <path to directory containing train data>  <path to dump the training data as .csv file>\n\t" \
                 "validate\n\t\t-v  <path to .dat file>\n\t" \
                 "train\n\t\t-t  <path to .dat file>  <path to dump model as .model file\n\t" \
                 "predict\n\t\t-p  <path to directory containing test data>  <path to .model file>\n\n" \
                 "Example:\n\tpython static.py -p /home/data/test/static /home/model/static.model\n\n" \
                 "Note:\n\tdata dump - creates a .dat file in same directory as .csv file with same name\n\tpredict   - creates output.csv containing all samples with their respective tags (M/B)\n\n" \
                 "Warning: Please use only one option at a time to avoid any unexpected behavior.\n"
    elif category == "data":
        string += "Missing data path(s)\nPlease use the below format to execute the script\n\nUsage: python static.py [OPTION]... [FILE]...\n\n" \
                  "data dump: (also creates a .dat file in same directory as .csv file with same name)\n\t\t-d  <path to directory containing train data>  <path to dump the training data as csv>\n" \
                  "Example:\n\tpython static.py -d /home/data/train/static /home/data/train/static.csv\n"
    elif category == "validate":
        string += "Missing .dat file\nPlease use the below format to execute the script\n\nUsage: python static.py [OPTION]... [FILE]...\n\n" \
                  "validate: \n\t\t-v  <path to .dat file>\n" \
                  "Example:\n\tpython static.py -v /home/data/train/static.dat\n"
    elif category == "train":
        string += "Missing data path(s)\nPlease use the below format to execute the script\n\nUsage: python static.py [OPTION]... [FILE]...\n\n" \
                  "train:\n\t\t-t  <path to .dat file>  <path to dump model as .model file>\n" \
                  "Example:\n\tpython static.py -t /home/data/train/static.dat /home/data/train/static.model\n"
    elif category == "predict":
        string += "Missing data path(s)\nPlease use the below format to execute the script\n\nUsage: python static.py [OPTION]... [FILE]...\n\n" \
                 "predict:\n\t\t-p  <path to directory containing test data>  <path to .model file>\n\n" \
                 "Example:\n\tpython static.py -p /home/data/test/static /home/model/static.model\n"
    print(string)


'''
main functions
'''

'''
@input
    filedir_path    - absolute path to the directory containing data files related to the sample
@output
    True/ False
@function
    checks whether the given sample is packed or non-packed based on certain heuristics
'''
def isPacked(filedir_path):
    with open(os.path.join(filedir_path, "Structure_Info.txt"), 'rb') as fh:
        ispacked = True
        lines = fh.readlines()
        for index, line in enumerate(lines):
            line = line.decode('utf8', 'ignore').split()
            if ('.text' in line or 'CODE' in line):
                ispacked = False
                # mark this line no
                marked_line = index
                break
        # even with .text section, it might still be packed
        if not ispacked:
            # vSize/rSize mentioned 4/6 lines below
            vSize_line = lines[marked_line + 4].decode('utf8', 'ignore').split()
            rSize_line = lines[marked_line + 6].decode('utf8', 'ignore').split()
            # read hex string to integer - significant size difference between raw size and virtual size
            if (int(rSize_line[3], 16) < int(vSize_line[3], 16) / 16):
                ispacked = True
        return ispacked


'''
@input
    filedir_path    - absolute path to the directory containing data files related to the sample
@output
    list of APIs in given sample
@function
    extracts all relavant APIs feature information from the data files related to the sample and builds a list of APIs
'''
def find_apis(filedir_path):
    api_list = []
    # .../Malware/Trojan/0aeb1...
    with open(os.path.join(filedir_path, "Structure_Info.txt"), 'rb') as fh:
        lines = fh.readlines()
        for line in lines:
            line = line.decode('utf8', 'ignore')
            try:
                api = line.split()[0].split('.')
                if 'dll' == api[1].lower():
                    api_list.append(api[2])
            except:
                pass
        return api_list


'''
@input
    filedir_path    - absolute path to the directory containing data files related to the sample
@output
    dictionary of features
@function
    extracts all relavant API feature information from the data files related to the sample and builds a dictionary
'''
def get_api_features(filedir_path):
    features_dict = dict.fromkeys(api_features, 0)
    api_list = find_apis(filedir_path)
    for api in api_list:
        if features_dict.get(api, -1) != -1:
            features_dict[api] += 1
    return features_dict


'''
@input
    filedir_path    - absolute path to the directory containing data files related to the sample
@output
    dictionary of features
@function
    extracts all relavant structural feature information from the data files related to the sample and builds a dictionary
'''
def get_str_features(filedir_path):
    raw_features_dict = dict.fromkeys(raw_features, 0)
    derived_features_dict = dict.fromkeys(derived_features, 0)
    # parse file to get the third field
    with open(os.path.join(filedir_path, "Structure_Info.txt"), 'rb') as fh:
        lines = fh.readlines()
        for index, line in enumerate(lines):
            line = line.decode('utf8', 'ignore').split()
            # process it, if is in list of features
            try:
                field = line[2][:-1]
                if field in raw_features:
                    # convert to integer & store
                    value = int(line[3], 16)
                    raw_features_dict[field] = value
            except:
                try:
                    if line[0] == aux_derived[0]:
                        derived_features_dict['StringFileInfo'] = 1
                    elif line[0] == aux_derived[1]:
                        value = int(lines[index + 7].split()[3], 16)
                        derived_features_dict['Characteristics'] = value
                        # print("Found: ", derived_features_dict['Characteristics'])
                    elif line[0] == aux_derived[2]:
                        value = int(lines[index + 24].split()[3], 16)
                        derived_features_dict['DllCharacteristics'] = value
                except:
                    continue
            continue
        return { **raw_features_dict, **derived_features_dict }


'''
@input
    filedir_path    - absolute path to the directory containing data files related to the sample
@output
    dictionary of features
@function
    extracts all relavant feature information from the data files related to the sample and builds a dictionary
'''
def extract_features(filedir_path):
    str_features_dict = get_str_features(filedir_path)
    api_features_dict = get_api_features(filedir_path)
    return { **str_features_dict, **api_features_dict }


'''
@input
    dir_path        - absolute path to the parent directory containing Malware/ Benign sample folders, see structure below
    |
    --- Malware
        |
        --- Trojan
            |
            --- file_dir
                |
                --- Structure_Info.txt
        ...
    --- Benign
        |
        --- Only
            |
            --- file_dir
            ...
    [saveto_path]   - path to save the created data.csv
    [timed]         - pretty print status of execution (default 5, i.e., prints after every 5% of the files are processed)
@function
    creates a .csv file containing feature vectors of all samples
'''
def dump_data_static(dir_path,  saveto_path="Default", timed=5):
    if saveto_path == 'Default':
        saveto_path = replace_ext(dir_path, '.csv')
    aux_features_dict = { 'IsPacked' : 0, 'Label' : 0}
    field_names = str_features + api_features + list(aux_features_dict.keys())
    print("Processing files...")
    file_count = sum(len(files) for _, _, files in os.walk(dir_path)) / 3
    steps = int((file_count * timed) / 100)
    with open(saveto_path, 'w') as fh:
        csv_writer = csv.DictWriter(fh, fieldnames=field_names)
        # feature_names
        csv_writer.writeheader()
        count = 0
        percentage = timed
        for sup_dir in os.listdir(dir_path):
            if sup_dir == 'Malware':
                aux_features_dict['Label'] = TYPE.MALWARE.value
            else:
                aux_features_dict['Label'] = TYPE.BENIGN.value
            # .../Malware or .../Benign
            sup_dir = os.path.join(dir_path, sup_dir)
            for sub_dir in os.listdir(sup_dir):
                feature_list = []
                # .../Malware/Trojan or .../Benign/Only
                sub_dir = os.path.join(sup_dir, sub_dir)
                for file_dir in os.listdir(sub_dir):
                    count += 1
                    if count % steps == 0 and percentage < 100:
                        print("...%d%%" % (percentage), end='', flush=True)
                        percentage += timed
                    # .../Malware/Trojan/0aeb1...
                    abs_file_dir_path = os.path.join(sub_dir, file_dir)
                    if isPacked(abs_file_dir_path):
                        aux_features_dict['IsPacked'] = 1
                    else:
                        aux_features_dict['IsPacked'] = 0
                    feature_list.append({ **extract_features(abs_file_dir_path), **aux_features_dict})
                csv_writer.writerows(feature_list)
    print("...100%\nCompleted\n")
    print("Dumping data to %s..."%(saveto_path))
    meta_data =[str(item) for item in [count, len(str_features) + len(api_features) + len(aux_features_dict) - 1]]
    with open(saveto_path, 'r') as fh:
        content = fh.read()
    with open(saveto_path, 'w') as fh:
        fh.write(','.join(meta_data) + '\n' + content)
    print("Done\n")


def main():
    arg_list = sys.argv

    try:
        data_dump = arg_list.index('-d')
    except ValueError:
        data_dump = 0
    try:
        val_model = arg_list.index('-v')
    except ValueError:
        val_model = 0
    try:
        train_model = arg_list.index('-t')
    except ValueError:
        train_model = 0
    try:
        predict_model = arg_list.index('-p')
    except ValueError:
        predict_model = 0
    try:
        help = arg_list.index('--help')
    except ValueError:
        help = 0

    if help or not (data_dump or val_model or train_model or predict_model):
        display_help()

    # build data (CSV) and dataset
    if data_dump:
        # specify paths to data directory & data (CSV) save directory
        try:
            path_to_data_dir = arg_list[data_dump + 1]
            path_to_data_dump = arg_list[data_dump + 2]
        except:
            display_help("data")
            exit(1)
        dump_data_static(path_to_data_dir, path_to_data_dump)
        path_to_dat_dump = create_dataset(path_to_data_dump)

    # k-cross validation
    if val_model:
        # specify path to data.dat only if data_set wasn't created during this run
        try:
            path_to_dat_dump = arg_list[val_model + 1]
        except:
            display_help("validate")
            exit(1)
        clf = RandomForestClassifier(n_estimators=100)
        train_data = load_dataset(path_to_dat_dump)
        X = train_data.data
        y = train_data.target
        k_cross_validate(X, y, clf, k=5)

    # build data.model
    if train_model:
        # specify path to data.dat only if data_set wasn't created during this run
        try:
            path_to_dat_dump = arg_list[train_model + 1]
            path_to_model_dump = arg_list[train_model + 2]
        except:
            display_help("train")
            exit(1)
        train_data = load_dataset(path_to_dat_dump)
        X = train_data.data
        y = train_data.target
        clf = RandomForestClassifier(n_estimators=100)
        path_to_model = train(X, y, clf, path_to_model_dump)

    # predict based on data.model
    if predict_model:
        try:
            path_to_data_dir = arg_list[predict_model + 1]
            path_to_model = arg_list[predict_model + 2]
        except:
            display_help("predict")
            exit(1)
        with open(path_to_model, 'rb') as f:
            model = pickle.load(f)
        aux_features_dict = {'IsPacked': 0}
        print("Processing Files...")
        file_count = sum(len(files) for _, _, files in os.walk(path_to_data_dir)) / 3
        steps = int((file_count * 10) / 100)
        count = 0
        percentage = 10
        with open("output.csv", 'w') as fh:
            fh.write("File_Hash,Predicted Label")
            file_name_list = []
            feature_vec_list = []
            for file_dir in os.listdir(path_to_data_dir):
                count += 1
                if count % steps == 0 and percentage < 100:
                    print("...%d%%" % (percentage), end='', flush=True)
                    percentage += 10
                # .../Malware/Trojan/0aeb1...
                abs_file_dir_path = os.path.join(path_to_data_dir, file_dir)
                if isPacked(abs_file_dir_path):
                    aux_features_dict['IsPacked'] = 1
                else:
                    aux_features_dict['IsPacked'] = 0
                feature_dict = {**extract_features(abs_file_dir_path), **aux_features_dict}
                feature_vec = []
                for feature in feature_names:
                    feature_vec.append(feature_dict[feature])
                feature_vec_list.append(feature_vec)
                file_name_list.append(file_dir)
            print("...100%\nCompleted\n")
            print("Dumping results to output.csv...")
            test_data = np.array(feature_vec_list, dtype=np.int)
            y = model.predict(test_data)
            label_list = list(map(lambda i: 'M' if i == 1 else 'B', y))
            for item in zip(file_name_list, label_list):
                fh.write("\n" + item[0] + "," + item[1])
            print("Done\n")


if __name__ == '__main__':
    main_timer = timer()
    main_timer.start()
    main()
    main_timer.end()
    main_timer.show()
