# Title / Mid-sem submission

## About / Folder containing relavant script and model used to tag given samples as M/B (Malware/Benign). An auxiliary folder containing full script and other auxiliary information.


## Execution

Sample prediction:

* python dynamic.py -p /home/data/test/dynamic /home/model/dynamic.model

In general it adheres to the following format for prediction on test data:

* python dynamic.py -p <path to directory containing test data> <path to .model file>


## Command Line Arguments

There are other options as well which take the form python dynamic.py [OPTION]... [FILE]...

Options:
	data dump
		-d  <path to directory containing train data>  <path to dump the training data as .csv file>
	validate
		-v  <path to .dat file>
	train
		-t  <path to .dat file>  <path to dump model as .model file
	predict
		-p  <path to directory containing test data>  <path to .model file>

Use only one option at a time.

More details on the same can be accessed by typing the following command:

* python dynamic.py --help


## Code

The script for running only the prediction can be found under the dynamic folder as dynamic.py. The full script is available under the misc folder as dynamic_full.py.

### Content

All of the sub-modules and functions are contained individually in each of the script files (and hence may see some redundancy). Mainly three categories of functions can be found - generic helper functions used to automate mundane tasks, auxiliary functions used to directly assist the main functions which are in turn used to process the input.

The code contains exhaustive documentation of each of the functions and the role they play.

At the outset, the main() function gets called and as per the options passed (described previously) invokes respective functions to get the task done. Specifically for the task of predicting labels, the get_features() function is invoked per file to extract all the relevant features. Once the features are obtained, they are written to an output file by name static.csv as per the prescribed format.

### Requirements

The code was written and tested in Python 3.7.4

For running only the prediction part, only the following modules were used:

> enum, os, sys, pickle, time, numpy

All of the following python packages would be required for executing the script with all its functions:

> enum, os, sys, pickle, time, numpy, sklearn, pandas, time, csv, json, feature_selector

If not found, these can be installed using standard pip install under the following names respectively:

* pip install [enum os-sys pickle-mixin times numpy scikit-learn pandas python-csv jsonlib feature_selector]


## Appendix

### Resources

The following were the main resourcesused in the entire process of building the classifier model starting from feature extraction, feature selection and finally classification itself.

> https://pdfs.semanticscholar.org/6528/f669cab80c94633432a71624afb8a9a82b5f.pdf
> https://ieeexplore.ieee.org/document/8667136
> https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection
> https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/
> https://pypi.org/project/feature-selector/
> Various other online resources and class material


### Process description

Referring to various online resources and class material, the following categories of features were chosen to be extracted from the json files of the training samples:

* API categories
* APIs
* network stats
* summary

Initially this amounted to about 324 features in total. This was further subject to feature selection (based on a specialised feature selection algorithm) resulting in 74 features in total.

#### Feature Selection

Two approaches were used to reduce the number of features - one based on the importance metrics output by a model which processed the data; another based on a feature selection algorithm (the aggregate outputs of both approaches helped in selecting the desired set of features).

The first one was based on the SelectFromModel module under sklearn. This gives a meta-transformer for selecting features based on importance weights. The second one was another module feature-selector which is a tool for dimensionality reduction for machine learning datasets. The latter helped in feature selection based on single-valuedness, correlation and relative importance of features. Links containing details of both can be found under the Resources section.

#### Model selection

From among the standard machine learning models for binary classification such as tree based approaches, bayesian networks, SVM etc, the tree based classifiers produced best results. They are especially robust to over-fitting (since a large number of trees cancel out the negative effect). Among these RandomForestClassifier from the scikit-learn package was used.

#### Intermediate results

The details of all the features initially chosen and the intermediate features as described above can be found under misc/dynamic_features_list.txt
