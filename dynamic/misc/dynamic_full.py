import csv
import enum
import json
import os
import pickle
import time
import sys
import numpy as np
import pandas as pd

from sklearn import metrics
from sklearn.datasets.base import Bunch
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import KFold

from feature_selector import FeatureSelector

'''
helper classes
'''
class timer():

    def start(self):
        self.start_time = time.time()

    def end(self):
        end_time = time.time()
        total_time = self.start_time - end_time
        self.hours = int(total_time / 3600)
        remaining_secs = total_time - self.hours * 3600
        self.minutes = int(remaining_secs / 60)
        self.seconds = remaining_secs - self.minutes * 60

    def show(self):
        print("Total Execution Time\n{}h : {}m : {}s".format(self.hours, self.minutes, int(self.seconds)))

class TYPE(enum.Enum):
    BENIGN = 0
    MALWARE = 1

api_info_dict = {'NtCreateSection': 'process', 'InternetGetConnectedStateExA': 'network', 'LoadStringA': 'ui',
                 'NtReadFile': 'file', 'DrawTextExA': 'ui', 'WriteConsoleW': 'misc', 'GetBestInterfaceEx': 'network',
                 'FindResourceExW': 'resource', 'RegQueryValueExA': 'registry', 'CopyFileW': 'file',
                 'DeleteFileW': 'file', 'DrawTextExW': 'ui', 'CreateServiceA': 'services', 'OpenServiceA': 'services',
                 'GetVolumeNameForVolumeMountPointW': 'file', 'GetAdaptersAddresses': 'network',
                 'WriteConsoleA': 'misc', 'CryptProtectMemory': 'crypto', 'WSAStartup': 'network',
                 'RegSetValueExW': 'registry', 'sendto': 'network', 'CertOpenSystemStoreA': 'certificate',
                 'NtDeleteValueKey': 'registry', 'SizeofResource': 'resource', 'InternetGetConnectedState': 'network',
                 'CryptExportKey': 'crypto', 'StartServiceW': 'services', 'EnumServicesStatusA': 'services',
                 'SearchPathW': 'file', 'NtWriteFile': 'file', 'CoGetClassObject': 'ole',
                 'CreateProcessInternalW': 'process', 'NtFreeVirtualMemory': 'process', 'UuidCreate': 'misc',
                 'NtAllocateVirtualMemory': 'process', 'SetFileTime': 'file', 'CreateRemoteThreadEx': 'process',
                 'InternetQueryOptionA': 'network', 'GetKeyboardState': 'system', 'GetForegroundWindow': 'ui',
                 'CertCreateCertificateContext': 'certificate', 'DnsQuery_W': 'network', 'HttpSendRequestA': 'network',
                 'RegEnumKeyW': 'registry', 'RegEnumKeyExW': 'registry', 'NtCreateFile': 'file',
                 'RegDeleteValueA': 'registry', 'FindWindowExA': 'ui', 'Process32FirstW': 'process',
                 'GetTimeZoneInformation': 'misc', 'RtlCreateUserThread': 'process', 'SetErrorMode': 'system',
                 'CreateJobObjectW': 'process', 'RtlDecompressBuffer': 'system', 'CoInitializeSecurity': 'ole',
                 'NtSetValueKey': 'registry', 'SendNotifyMessageW': 'system', 'InternetCrackUrlA': 'network',
                 'CryptDecodeObjectEx': 'crypto', 'RegisterHotKey': 'misc', 'NtDeleteKey': 'registry',
                 '__exception__': '__notification__', 'InternetOpenA': 'network', 'EnumServicesStatusW': 'services',
                 'NtEnumerateValueKey': 'registry', 'GetCursorPos': 'misc', 'IWbemServices_ExecQuery': 'misc',
                 'NtMapViewOfSection': 'process', 'GetFileSizeEx': 'file', 'send': 'network', 'setsockopt': 'network',
                 'GetSystemTimeAsFileTime': 'synchronisation', 'GetTempPathW': 'file', 'CryptUnprotectMemory': 'crypto',
                 'NtOpenSection': 'process', 'GetSystemWindowsDirectoryA': 'file', 'recv': 'network',
                 'SetFileAttributesW': 'file', 'NtUnmapViewOfSection': 'process', 'NtSuspendThread': 'process',
                 'GetSystemDirectoryA': 'file', 'SetInformationJobObject': 'process', 'NtSetContextThread': 'process',
                 'ShellExecuteExW': 'process', 'NtClose': 'system', 'NtWriteVirtualMemory': 'process',
                 'NtQueryDirectoryFile': 'file', 'CryptHashData': 'crypto', 'InternetSetOptionA': 'network',
                 'FindWindowA': 'ui', 'FindFirstFileExW': 'file', 'CopyFileExW': 'file', 'IsDebuggerPresent': 'system',
                 'AssignProcessToJobObject': 'process', 'LoadStringW': 'ui', 'WSARecv': 'network',
                 'LookupPrivilegeValueW': 'system', 'NtQueryMultipleValueKey': 'registry', 'RegDeleteKeyA': 'registry',
                 'GetDiskFreeSpaceW': 'misc', 'RegCreateKeyExA': 'registry', 'NtOpenMutant': 'synchronisation',
                 'HttpOpenRequestW': 'network', 'NtQueryAttributesFile': 'file', 'MessageBoxTimeoutW': 'ui',
                 'GetSystemInfo': 'system', 'NtTerminateProcess': 'process', 'gethostbyname': 'network',
                 'GetUserNameW': 'misc', 'CoCreateInstance': 'ole', 'GetComputerNameW': 'misc',
                 'LookupAccountSidW': 'misc', 'NtResumeThread': 'process', 'SetFilePointerEx': 'file',
                 'HttpOpenRequestA': 'network', 'select': 'network', 'RegEnumValueW': 'registry', 'accept': 'network',
                 'GetSystemMetrics': 'misc', 'InternetOpenUrlA': 'network', 'WSASocketW': 'network',
                 'LoadResource': 'resource', 'CryptAcquireContextW': 'crypto', 'closesocket': 'network',
                 'IWbemServices_ExecMethod': 'misc', 'RegCreateKeyExW': 'registry', 'FindWindowW': 'ui',
                 'WriteProcessMemory': 'process', 'CreateDirectoryExW': 'file', 'HttpQueryInfoA': 'network',
                 'OpenSCManagerW': 'services', 'GetUserNameA': 'misc', 'NtGetContextThread': 'process',
                 'FindWindowExW': 'ui', 'NtQueueApcThread': 'process', 'GetComputerNameA': 'misc',
                 'RegQueryInfoKeyA': 'registry', 'GetUserNameExA': 'misc', 'GetNativeSystemInfo': 'system',
                 'LdrLoadDll': 'system', 'NtReadVirtualMemory': 'process', 'RegDeleteValueW': 'registry',
                 'RegEnumValueA': 'registry', 'GetAddrInfoW': 'network', 'RtlAddVectoredExceptionHandler': 'exception',
                 'OpenSCManagerA': 'services', 'NetShareEnum': 'netapi', 'listen': 'network',
                 'RegSetValueExA': 'registry', 'FindResourceExA': 'resource', 'CoCreateInstanceEx': 'ole',
                 'socket': 'network', 'InternetConnectW': 'network', 'NtQuerySystemInformation': 'system',
                 'GetFileInformationByHandleEx': 'file', 'CertControlStore': 'certificate',
                 'ObtainUserAgentString': 'network', 'SetFileInformationByHandle': 'file', 'SetFilePointer': 'file',
                 'CoUninitialize': 'ole', 'CryptAcquireContextA': 'crypto', 'GetFileVersionInfoSizeExW': 'misc',
                 'RegCloseKey': 'registry', 'WSASocketA': 'network', 'TaskDialog': 'misc',
                 'GetDiskFreeSpaceExW': 'misc', 'NetUserGetInfo': 'netapi', 'NtQueryInformationFile': 'file',
                 'GetFileVersionInfoW': 'misc', 'FindFirstFileExA': 'file', 'WSASendTo': 'network',
                 'InternetCloseHandle': 'network', 'GetVolumePathNameW': 'file', 'GetFileVersionInfoSizeW': 'misc',
                 'recvfrom': 'network', 'GetFileInformationByHandle': 'file', 'Module32FirstW': 'process',
                 'NtQueryValueKey': 'registry', 'OpenServiceW': 'services', 'LdrGetDllHandle': 'system',
                 'SetWindowsHookExA': 'system', 'NtDelayExecution': 'synchronisation', 'NtCreateThreadEx': 'process',
                 'GetFileSize': 'file', 'EnumWindows': 'misc', 'NtTerminateThread': 'process',
                 'RtlAddVectoredContinueHandler': 'exception', 'FindResourceW': 'resource', 'bind': 'network',
                 'GetFileVersionInfoExW': 'misc', 'GetKeyState': 'system', 'GlobalMemoryStatus': 'system',
                 'LdrGetProcedureAddress': 'system', 'StartServiceA': 'services', 'SendNotifyMessageA': 'system',
                 'InternetSetStatusCallback': 'network', 'NtOpenKeyEx': 'registry', 'GetShortPathNameW': 'file',
                 'Thread32Next': 'process', 'WSARecvFrom': 'network', 'CreateToolhelp32Snapshot': 'process',
                 'RtlRemoveVectoredExceptionHandler': 'exception', 'NtOpenFile': 'file',
                 'GetVolumePathNamesForVolumeNameW': 'file', 'OutputDebugStringA': 'system',
                 'RegOpenKeyExA': 'registry', 'DeviceIoControl': 'file', 'MoveFileWithProgressW': 'file',
                 'NtEnumerateKey': 'registry', 'shutdown': 'network', 'NtOpenDirectoryObject': 'file',
                 'ioctlsocket': 'network', 'UnhookWindowsHookEx': 'system', 'InternetOpenUrlW': 'network',
                 'getaddrinfo': 'network', 'Module32NextW': 'process', 'HttpSendRequestW': 'network',
                 'NtSetInformationFile': 'file', 'GetInterfaceInfo': 'network', 'CreateServiceW': 'services',
                 'GetFileType': 'file', 'CoInitializeEx': 'ole', 'NtDeviceIoControlFile': 'file',
                 'GetSystemDirectoryW': 'file', 'RegQueryValueExW': 'registry', 'RemoveDirectoryA': 'file',
                 'GetFileAttributesExW': 'file', 'RegQueryInfoKeyW': 'registry', 'getsockname': 'network',
                 'NtOpenKey': 'registry', 'GlobalMemoryStatusEx': 'system', 'WNetGetProviderNameW': 'network',
                 'CreateRemoteThread': 'process', 'RemoveDirectoryW': 'file', 'GetAdaptersInfo': 'network',
                 'NtQueryKey': 'registry', 'InternetConnectA': 'network', 'ControlService': 'services',
                 'RegOpenKeyExW': 'registry', 'GetAsyncKeyState': 'system', 'PRF': 'crypto',
                 'FindResourceA': 'resource', 'CreateDirectoryW': 'file', 'NtProtectVirtualMemory': 'process',
                 'SHGetFolderPathW': 'misc', 'GetSystemWindowsDirectoryW': 'file', 'GetUserNameExW': 'misc',
                 'CryptCreateHash': 'crypto', 'ReadCabinetState': 'misc', 'CryptGenKey': 'crypto',
                 'DeleteUrlCacheEntryW': 'network', 'RegEnumKeyExA': 'registry', 'ReadProcessMemory': 'process',
                 'CryptProtectData': 'crypto', 'DeleteService': 'services', 'CertOpenStore': 'certificate',
                 'NtOpenThread': 'process', 'NtCreateKey': 'registry', 'CopyFileA': 'file',
                 'NtDuplicateObject': 'system', 'NtQueryFullAttributesFile': 'file', 'Thread32First': 'process',
                 'NetGetJoinInformation': 'netapi', 'InternetOpenW': 'network', 'timeGetTime': 'synchronisation',
                 'ExitWindowsEx': 'system', 'CreateActCtxW': 'misc', 'OleInitialize': 'ole',
                 'InternetCrackUrlW': 'network', 'RegDeleteKeyW': 'registry', 'connect': 'network',
                 'SetStdHandle': 'misc', 'LdrUnloadDll': 'system', 'MessageBoxTimeoutA': 'ui',
                 'SetUnhandledExceptionFilter': 'exception', 'system': 'process', 'NtLoadDriver': 'system',
                 'Process32NextW': 'process', 'SetWindowsHookExW': 'system', 'SetEndOfFile': 'file',
                 'NtCreateMutant': 'synchronisation', 'NtOpenProcess': 'process', 'SHGetSpecialFolderLocation': 'misc',
                 'CreateThread': 'process', 'GetFileAttributesW': 'file', 'CryptDecrypt': 'crypto'}

'''
reduced feature set after feature selection - total 74
'''
api_categories =  ['crypto', 'ole', 'ui', 'process', 'file', '__notification__', 'exception', 'registry', 'system', 'synchronisation', 'resource']
api_stats_list =  ['__exception__', 'SearchPathW', 'DrawTextExW', 'FindResourceExW', 'NtQueryDirectoryFile', 'GetSystemMetrics', 'LoadStringW', 'GetFileSizeEx', 'FindResourceW', 'LdrUnloadDll', 'NtQueryAttributesFile', 'CoUninitialize', 'GetFileSize', 'GetFileInformationByHandleEx', 'NtOpenKeyEx', 'NtQueryValueKey', 'GetFileType', 'LdrGetDllHandle', 'NtOpenKey', 'NtAllocateVirtualMemory', 'RegOpenKeyExW', 'RegCloseKey', 'RegQueryValueExW', 'GetSystemTimeAsFileTime', 'NtClose', 'NtMapViewOfSection', 'GetSystemDirectoryW', 'NtWriteFile', 'NtCreateFile', 'GetFileVersionInfoW', 'NtOpenMutant', 'LdrGetProcedureAddress', 'GetFileAttributesW', 'LoadResource', 'SetFilePointer', 'GetSystemWindowsDirectoryW', 'SizeofResource', 'GetForegroundWindow', 'CoInitializeEx', 'WriteConsoleW', 'GetCursorPos', 'NtReadFile', 'NtCreateSection', 'LdrLoadDll']
network =  ['http', 'tcp', 'icmp', 'dns', 'hosts', 'udp', 'domains']
summary =  ['file_deleted', 'file_opened', 'regkey_written', 'guid', 'directory_enumerated', 'regkey_opened', 'file_read', 'dll_loaded', 'file_exists', 'regkey_read', 'directory_created']
feature_names = ['duration'] + network + api_categories + summary + api_stats_list

'''
generic helper functions
'''

'''
@input
    file_path   - absolute file path
    ext         - extension to replace existing one
@output
    default file_path obtained by replacing default extension of file_path with ext
@function
    helps generate default file_path (which is used to save useful information output by other functions)
'''
def replace_ext(file_path, ext):
    (dir_path, file_name) = os.path.split(file_path)
    return dir_path + os.path.sep + str(file_name.split('.')[0]) + ext


'''
@input
    data_file_path  - input .csv file to process (with meta_data in first row)
    [saveto_path]   - absolute path to save the modified .csv file
@output
    path to modified .csv file (with meta_data removed)
@function
    removes first line of .csv file (containing meta_data) and dumps modified data to another .csv file
'''
def strip_header(data_file_path, saveto_path="Default"):

    if saveto_path == "Default":
        saveto_path = replace_ext(data_file_path, '_mod.csv')

    with open(data_file_path, 'r') as fh:
        fh.readline()
        contents = fh.read()
    with open(saveto_path, 'w') as fh:
        fh.write(contents)

    return saveto_path


'''
@input
    data_file_path  - absolute path to the .csv file containing original train data (features and labels)
    [saveto_path]   - absolute path to the updated .dat file (defaults to '*.dat' in the current directory, * corresponding to original file name)
@output
    dumps the train data to a .dat file and returns absolute path to it
@function
    extracts all training data (features and labels) and packs it into a dataset of numpy array and other auxiliary information (for conveniently loading later)   
'''
def create_dataset(data_file_path, saveto_path="Default"):
    if saveto_path == "Default":
        saveto_path = replace_ext(data_file_path, '.dat')
    with open(data_file_path) as fh:
        file_data = csv.reader(fh)
        # dimensions
        meta_data = next(file_data)
        feature_names = next(file_data)
        # number of data rows, excluding header
        num_samples = int(meta_data[0])
        # number of columns for features, excluding target column
        num_features = int(meta_data[1])
        target_names = ['Benign', 'Malware']
        data = np.empty((num_samples, num_features))
        target = np.empty((num_samples,), dtype=np.int)
        for ind, sample in enumerate(file_data):
            data[ind] = np.asarray(sample[:-1], dtype=np.float64)
            target[ind] = np.asarray(sample[-1], dtype=np.int)
        dataset = Bunch(data=data, target=target, feature_names=feature_names, target_names=target_names,
                        shape=(num_samples, num_features))
        with open(saveto_path, 'wb') as fh1:
            pickle.dump(dataset, fh1)
        return saveto_path


'''
@input
    dat_file    - absolute path to .dat file containing the training data
@output
    the loaded dataset (containing features, labels and other auxialiary information)
'''
def load_dataset(dat_file):
    with open(dat_file, 'rb') as fh:
        data = pickle.load(fh)
    return data


'''
auxiliary functions
'''

'''
@input
    data_file_path  - input .csv file to process
    [saveto_path]   - absolute path to save the modified .csv file
@function   
    reduces the number of features by elimination based on correlation, importance of features and dumps modified data to another .csv file
'''
def select_best_features(data_file_path, saveto_path="Default"):
    mod_data_file_path = strip_header(data_file_path)

    if saveto_path == "Default":
        saveto_path = replace_ext(data_file_path, '_reduced.csv')

    X = pd.read_csv(mod_data_file_path)
    y = X['Label']
    X = X.drop(columns=['Label'])

    feature_selector = FeatureSelector(data=X, labels=y)
    feature_selector.identify_single_unique()
    feature_selector.identify_collinear(correlation_threshold=0.99)
    feature_selector.identify_zero_importance(task = 'classification', eval_metric = 'auc',
                            n_iterations = 10, early_stopping = True)
    features_1hot = feature_selector.one_hot_features
    features_base = feature_selector.base_features
    feature_selector.identify_low_importance(cumulative_importance=0.99)

    X_dash = feature_selector.remove(methods= ['single_unique', 'collinear', 'zero_importance', 'low_importance'], keep_one_hot=False)
    X_dash['Label'] = y

    X_dash.to_csv(saveto_path, index=False)

    meta_data = [str(X_dash.shape[0]), str(X_dash.shape[1] - 1)]
    with open(saveto_path, 'r') as fh:
        contents = fh.read()
    contents = ','.join(meta_data) + '\n' + contents
    with open(saveto_path, 'w') as fh:
        fh.write(contents)

    os.system("rm -f " + mod_data_file_path)


'''
@input
    dat_file_path   - absolute path to the .dat file containing train data (features and labels)
@output
    list of indices of the selected (reduced) feature set in correspondance to the original feature set
@function
    given a dataset of training features and labels, performs feature selection to reduce number of features
    (particularly in our case we reduce number of features from 342 to 84)
'''
def select_important_features(dat_file_path):
    train_data = load_dataset(dat_file_path)
    X = train_data.data
    y = train_data.target
    (row, col) = X.shape
    test = SelectFromModel(estimator=RandomForestClassifier(n_estimators=100)).fit(X, y)
    no_feat = len(np.where(test.get_support() == True)[0])
    print(no_feat)
    i = 0
    new_no_feat = no_feat
    while i < 3:
        i += 1
        new_no_feat = no_feat * i
        if no_feat > col:
            new_no_feat = no_feat * (i - 1)
            break
    if new_no_feat > 75:
        new_no_feat = 75
    # test_ = SelectFromModel(estimator=RandomForestClassifier(n_estimators=100), threshold=-np.inf, max_features=new_no_feat).fit(X, y)
    test_ = SelectFromModel(estimator=RandomForestClassifier(n_estimators=100)).fit(X, y)
    select_feature_indices = np.where(test_.get_support() == True)[0]
    return select_feature_indices


'''
@input
    file_path   - absolute path of sample file to be analyzed
@function
    updates a global set with all sub-fields under the summary field (which is in turn under the behavior field)  
'''
summaries = set()
def update_summaries(file_path):
    with open(file_path) as fh:
        data = json.load(fh)
        for key in data['behavior']['summary'].keys():
            summaries.add(key)


'''
@input
    file_path   - absolute path of sample file to be analyzed
@function
    checks if all APIs map to a unique category while updating a global dictionary mapping APIs to their category
@exit
    funcion exits if an API doesn't map to a unique category (across sample files)
'''
api_stats_dict = dict.fromkeys(api_stats_list, "")
def update_api_stats_dict(file_path):
    with open(file_path, 'r')  as fh:
        data = json.load(fh)
        for proc_dict in data['behavior']['processes']:
            for call_dict in proc_dict['calls']:
                val = api_stats_dict.get(str(call_dict['api']), -1)
                if val == -1 or val == "":
                    api_stats_dict[str(call_dict['api'])] = str(call_dict['category'])
                elif api_stats_dict[str(call_dict['api'])] == str(call_dict['category']):
                    continue
                else:
                    print(call_dict['api'], " doesn't map to a unique category")
                    exit(0)


'''
@input
    file_path   - absolute path of sample file to be analyzed
@output
    a set of all categories of APIs invoked
@function
    retrieves all categories of APIs invoked from the sample json report
'''
def get_api_categories(file_path):
    api_categories_set = set()
    with open(file_path, 'r')  as fh:
        data = json.load(fh)
        for proc_dict in data['behavior']['processes']:
            for call_dict in proc_dict['calls']:
                api_categories_set.add(call_dict['category'])
    return api_categories_set


'''
@input
    file_path   - absolute path of sample file to be analyzed
@output
    a set of all APIs invoked
@function
    retrieves all APIs invoked from the sample json file
'''
def get_api_stats(file_path):
    api_stats_set = set()
    with open(file_path, 'r')  as fh:
        data = json.load(fh)
        for api_dict in data['behavior']['apistats'].keys():
            for api in data['behavior']['apistats'][api_dict].keys():
                api_stats_set.add(api)
    return api_stats_set


'''
@input
    dir_path        - see structure below
    |
    --- Malware
        |
        --- Trojan
            |
            --- file_dir
                |
                --- Structure_Info.txt
        ...
    --- Benign
        |
        --- Only
            |
            --- file_dir
            ...
    [saveto_path]   - absolute file path to save statistics (default 'dump_api_info.txt' in the current directory)
    [timed]         - pretty print status of execution (default 5, i.e., prints after every 5% of the files are processed)
@function
    computes various statistics from the given samples (report.json files) - all APIs referenced and their categories, 
    all sub-fields under summary field under behavior statistics in report.json and outputs to a .txt file
'''
def dump_api_info_dynamic(dir_path, saveto_path="dump_api_info.txt", timed=5):
    print("Processing files...")
    file_count = sum(len(files) for _, _, files in os.walk(dir_path))
    steps = int((file_count * timed) / 100)
    api_categories = set()
    api_stats = set()
    count = 0
    percentage = timed
    for sup_dir in os.listdir(dir_path):
        # .../Malware or .../Benign
        sup_dir = os.path.join(dir_path, sup_dir)
        for sub_dir in os.listdir(sup_dir):
            # .../Malware/Trojan or .../Benign/Only
            sub_dir = os.path.join(sup_dir, sub_dir)
            for file in os.listdir(sub_dir):
                count += 1
                if count % steps == 0 and percentage < 100:
                    print("...%d%%" % (percentage), end='', flush=True)
                    percentage += timed
                # .../Malware/Trojan/0aeb1...
                abs_file_path = os.path.join(sub_dir, file)
                result = get_api_categories(abs_file_path)
                for item in result:
                    api_categories.add(item)
                result = get_api_stats(abs_file_path)
                for item in result:
                    api_stats.add(item)
                update_api_stats_dict(abs_file_path)
                update_summaries(abs_file_path)
    print("...100%\nCompleted\n")
    print("Writing results to", saveto_path, "...")
    with open(saveto_path, 'w') as fh:
        text = str(len(api_categories)) + "\napi_categories = " + str(api_categories) + "\n\n" + str(len(api_stats)) + \
               "\napi_stats = " + str(api_stats) + "\n\n" + str(len(api_stats_dict)) + \
               "\napi_info_dict = " + str(api_stats_dict)
        fh.write(text)
    print("Done\n")


'''
@input
        X           - train data of features
        y           - train data of labels corresponding to X
        clf         - specified classifier on which to train
    [saveto_path]   - absolute path of file to dump the model to (defaults to 'data.model')
@output
    absolute path to the file containing dump of the model 
@function
    trains clf on given data (X, y) and dumps the model onto a file
'''
def train(X, y, clf, saveto_path="Default"):
    if saveto_path == 'Default':
        saveto_path = os.path.join(os.getcwd(), 'dynamic.model')
    clf.fit(X, y)
    with open(saveto_path, 'wb') as fh:
        pickle.dump(clf, fh)
    return saveto_path


'''
@input
    model_path  - absolute path to the file containing dump of the model
    X_test      - test data features
    y_test      - test data labels
@function
    predicts the labels on given test data features and prints prediction metrics 
'''
def predict_(model_path, X_test, y_test):
    with open(model_path, 'rb') as fh:
        model = pickle.load(fh)
    y_pred = model.predict(X_test)
    print("Accurate to:", metrics.accuracy_score(y_test, y_pred))


'''
@input
    dat_file   - absolute path to the file containing 
    k          - 'k' in k-cross validation (no of splits)
    clf        - classifier to validate on
@function
    implementation of standard k-cross validation
'''
def k_cross_validate(X, y, clf, k):
    k_fold = KFold(n_splits=k, shuffle=True, random_state=1)
    for train_ind, test_ind in k_fold.split(X):
        print("TRAIN:", train_ind, "TEST:", test_ind)
        X_train, X_test = X[train_ind], X[test_ind]
        y_train, y_test = y[train_ind], y[test_ind]
        model_path = train(X_test, y_test, clf, saveto_path='val.model')
        predict_(model_path, X_test, y_test)
        os.system("rm " + model_path)


'''
@function
    usage helper function
'''
def display_help(category=None):
    string = "Incorrect Usage: "
    if category == None:
        string = "Usage:\tpython dynamic.py [OPTION]... [FILE]...\nTag given data-set with labels M/B (Malware/Benign)\n\n" \
                 "--help  display this help info\n\nOptions:\n\t" \
                 "data dump\n\t\t-d  <path to directory containing train data>  <path to dump the training data as .csv file>\n\t" \
                 "validate\n\t\t-v  <path to .dat file>\n\t" \
                 "train\n\t\t-t  <path to .dat file>  <path to dump model as .model file\n\t" \
                 "predict\n\t\t-p  <path to directory containing test data>  <path to .model file>\n\n" \
                 "Example:\n\tpython dynamic.py -p /home/data/test/dynamic /home/model/dynamic.model\n\n" \
                 "Note:\n\tdata dump - creates a .dat file in same directory as .csv file with same name\n\tpredict   - creates output.csv containing all samples with their respective tags (M/B)\n\n" \
                 "Warning: Please use only one option at a time to avoid any unexpected behavior.\n"
    elif category == "data":
        string += "Missing data path(s)\nPlease use the below format to execute the script\n\nUsage: python dynamic.py [OPTION]... [FILE]...\n\n" \
                  "data dump: (also creates a .dat file in same directory as .csv file with same name)\n\t\t-d  <path to directory containing train data>  <path to dump the training data as csv>\n" \
                  "Example:\n\tpython dynamic.py -d /home/data/train/dynamic /home/data/train/dynamic.csv\n"
    elif category == "validate":
        string += "Missing .dat file\nPlease use the below format to execute the script\n\nUsage: python dynamic.py [OPTION]... [FILE]...\n\n" \
                  "validate: \n\t\t-v  <path to .dat file>\n" \
                  "Example:\n\tpython dynamic.py -v /home/data/train/dynamic.dat\n"
    elif category == "train":
        string += "Missing data path(s)\nPlease use the below format to execute the script\n\nUsage: python dynamic.py [OPTION]... [FILE]...\n\n" \
                  "train:\n\t\t-t  <path to .dat file>  <path to dump model as .model file>\n" \
                  "Example:\n\tpython dynamic.py -t /home/data/train/dynamic.dat /home/data/train/dynamic.model\n"
    elif category == "predict":
        string += "Missing data path(s)\nPlease use the below format to execute the script\n\nUsage: python dynamic.py [OPTION]... [FILE]...\n\n" \
                 "predict:\n\t\t-p  <path to directory containing test data>  <path to .model file>\n\n" \
                 "Example:\n\tpython dynamic.py -p /home/data/test/dynamic /home/model/dynamic.model\n"
    print(string)


'''
main functions
'''

'''
@misc
    list of all features to be extracted and structure of json file

    ================( value )================
    info dict 
        duration : value => value
    ================( count )================
    dropped list => length
    network (network) (known #) dict
        udp list => length
        http list
        ...
        icmp list
    behavior_dict (apis, apis categories, api summaries)
        processes list => length
        apistats (unknown #) dict
            x dict
                dict(api : count) => api_stats += count
            y dict
                dict(api : count)
            ...
            z dict
                dict(api : count)
        summary dict
            regkey_read list => length
            dll_loaded list
            ...
            file_exists list
@input
    file_path   - absolute path of sample file to be analyzed 
@output
    a dictionary of features with corresponding values
@function
    analyzes the given sample json file and extracts useful features from it
'''
def get_features(file_path):
    feature_dict = dict.fromkeys(feature_names, 0)
    with open(file_path, 'rb') as fh:
        try:
            data = json.load(fh)
            try:
                feature_dict['duration'] = data['info']['duration']
            except:
                pass
            for feat in network:
                try:
                    feature_dict[feat] = len(data['network'][feat])
                except:
                    pass
            try:
                behave_dict = data['behavior']
                apis = set()
                for key in behave_dict['apistats'].keys():
                    for api in behave_dict['apistats'][key].keys():
                        try:
                            apis.add(api)
                            count = behave_dict['apistats'][key][api]
                            if feature_dict.get(api, -1) != -1:
                                feature_dict[api] += count
                        except:
                            pass
                    for api in apis:
                        category = api_info_dict[api]
                        if feature_dict.get(category, -1) != -1:
                            feature_dict[category] += 1
                for feat in summary:
                    try:
                        feature_dict[feat] = len(behave_dict['summary'][feat])
                    except:
                        pass
            except:
                pass
        except:
            print("Error parsing: ", file_path, "\nPlease check if the file is in valid json format")
            exit(1)
    # cutting down on any spurious features read, preserving only selected features
    feature_vec = {}
    for feature in feature_names:
        feature_vec[feature] = feature_dict[feature]
    return feature_vec


'''
@input
    dir_path        - refer dump_api_info_dynamic(...) for description
    [saveto_path]   - absolute file path to save statistics (default 'dump_api_info.txt' in the current directory)
    [timed]         - pretty print status of execution (default 5, i.e., prints after every 5% of the files are processed)
@function
    aggregates various features from the given samples (report.json files) and dumps it in a .csv file, which would then
    contain features for all samples aggregated by the function (including the labels viz., 0 for Benign and 1 for Malware)
'''
def dump_data_dynamic(dir_path, saveto_path="Default", timed=5):
    if saveto_path == 'Default':
        saveto_path = replace_ext(dir_path, '.csv')
    aux_features_dict = {'Label': 0}
    field_names = feature_names + list(aux_features_dict.keys())
    print("Processing files...")
    file_count = sum(len(files) for _, _, files in os.walk(dir_path))
    steps = int((file_count * timed) / 100)
    with open(saveto_path, 'w') as fh:
        csv_writer = csv.DictWriter(fh, fieldnames=field_names)
        # feature_names
        csv_writer.writeheader()
        count = 0
        percentage = timed
        for sup_dir in os.listdir(dir_path):
            if sup_dir == 'Malware':
                aux_features_dict['Label'] = TYPE.MALWARE.value
            else:
                aux_features_dict['Label'] = TYPE.BENIGN.value
            # .../Malware or .../Benign
            sup_dir = os.path.join(dir_path, sup_dir)
            for sub_dir in os.listdir(sup_dir):
                feature_list = []
                # .../Malware/Trojan or .../Benign/Only
                sub_dir = os.path.join(sup_dir, sub_dir)
                for file in os.listdir(sub_dir):
                    count += 1
                    if count % steps == 0 and percentage < 100:
                        print("...%d%%" % (percentage), end='', flush=True)
                        percentage += timed
                    # .../Malware/Trojan/0aeb1...
                    abs_file_path = os.path.join(sub_dir, file)
                    feature_vec = {**get_features(abs_file_path), **aux_features_dict}
                    feature_list.append(feature_vec)
                csv_writer.writerows(feature_list)
    print("...100%\nCompleted\n")
    print("Dumping data to %s..."%(saveto_path))
    meta_data = [str(item) for item in [count, len(feature_names) + len(aux_features_dict) - 1]]
    with open(saveto_path, 'r') as fh:
        contents = fh.read()
    with open(saveto_path, 'w') as fh:
        fh.write(','.join(meta_data) + '\n' + contents)
    print("Done\n")


def main():

    arg_list = sys.argv

    try:
        data_dump = arg_list.index('-d')
    except ValueError:
        data_dump = 0
    try:
        val_model = arg_list.index('-v')
    except ValueError:
        val_model = 0
    try:
        train_model = arg_list.index('-t')
    except ValueError:
        train_model = 0
    try:
        predict_model = arg_list.index('-p')
    except ValueError:
        predict_model = 0
    try:
        help = arg_list.index('--help')
    except ValueError:
        help = 0

    if help or not (data_dump or val_model or train_model or predict_model):
        display_help()

    # build data (CSV) and dataset
    if data_dump:
        try:
            path_to_data_dir = arg_list[data_dump + 1]
            path_to_data_dump = arg_list[data_dump + 2]
        except:
            display_help("data")
            exit(1)
        dump_data_dynamic(path_to_data_dir, path_to_data_dump)
        path_to_dat_dump = create_dataset(path_to_data_dump)

    # k-cross validation
    if val_model:
        if val_model:
            try:
                path_to_dat_dump = arg_list[val_model + 1]
            except:
                display_help("validate")
                exit(1)
        clf = RandomForestClassifier(n_estimators=100)
        train_data = load_dataset(path_to_dat_dump)
        X = train_data.data
        y = train_data.target
        k_cross_validate(X, y, clf, k=5)

    # build data.model
    if train_model:
        try:
            path_to_dat_dump = arg_list[train_model + 1]
            path_to_model_dump = arg_list[train_model + 2]
        except:
            display_help("train")
            exit(1)
        train_data = load_dataset(path_to_dat_dump)
        X = train_data.data
        y = train_data.target
        clf = RandomForestClassifier(n_estimators=100)
        path_to_model = train(X, y, clf, path_to_model_dump)


    if predict_model:
        try:
            path_to_data_dir = arg_list[predict_model + 1]
            path_to_model = arg_list[predict_model + 2]
        except:
            display_help("predict")
            exit(1)
        with open(path_to_model, 'rb') as f:
            model = pickle.load(f)
        print("Processing Files...")
        file_count = sum(len(files) for _, _, files in os.walk(path_to_data_dir))
        steps = int(file_count / 10)
        count = 0
        percentage = 10
        with open("output.csv", 'w') as fh:
            fh.write("File_Hash,Predicted Label")
            file_name_list = []
            feature_vec_list = []
            for file_name in os.listdir(path_to_data_dir):
                count += 1
                if count % steps == 0 and percentage < 100:
                    print("...%d%%" % (percentage), end='', flush=True)
                    percentage += 10
                # .../Malware/Trojan/0aeb1...
                abs_file_path = os.path.join(path_to_data_dir, file_name)
                feature_dict = get_features(abs_file_path)
                feature_vec = []
                for feature in feature_names:
                    feature_vec.append(feature_dict[feature])
                feature_vec_list.append(feature_vec)
                file_name_list.append(file_name.split('.')[0])
            print("...100%\nCompleted\n")
            print("Dumping results to output.csv...")
            test_data = np.array(feature_vec_list, dtype=np.int)
            y = model.predict(test_data)
            label_list = list(map(lambda i: 'M' if i == 1 else 'B', y))
            for item in zip(file_name_list, label_list):
                fh.write("\n" + item[0] + "," + item[1])
            print("Done\n")


if __name__ == '__main__':
    main_timer = timer()
    main_timer.start()
    main()
    main_timer.end()
    main_timer.show()

